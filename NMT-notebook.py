# -*- coding: utf-8 -*-
"""fnlp_assignment2_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LM5ciVS8lBNGwK_YyV5wO_XG4DBjbxbw

## Assignment 2: Neural Machine Translation

In this assignment, we will implement an encoder-decoder neural network for the task of machine translation.

![img](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png)

We'll translate short image descriptions (without using the images themselves) from English to German.
This task shows the scale of machine translation while not requiring you to train your model for weeks if you don't use GPU.

### ❗  Contributions

**Please add a brief 1-2 sentence statement explaining the contributions of each group member (only refer to group members by UUNs):**

*< your contributions >*

## Setup
"""

# install dependencies
!pip3 install torch pandas nltk matplotlib
!pip3 install subword-nmt &> log
!wget https://gist.githubusercontent.com/saparina/45e4e8af5cb6b57c786ca246e1c75530/raw/b7d06c8a570b68ef22c9fc2f627bbb8a54f4cf27/vocab.py

# Commented out IPython magic to ensure Python compatibility.
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# %matplotlib inline

random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = 'cuda' if torch.cuda.is_available() else 'cpu'

source_lang = "en"
target_lang = "de"
splits = ['train', 'val', 'test']

data = {}
for split in splits:
    file_name = f"{split}.jsonl"
    df = pd.read_json(f"hf://datasets/bentrevett/multi30k/{file_name}", lines=True)
    data[split] = df

for split, df in data.items():
    df['de'].to_csv(f"{split}.de", index=False, header=False, mode='w')
    df['en'].to_csv(f"{split}.en", index=False, header=False, mode='w')

"""## 1. Data Preprocessing (5 points)

Before we get to the architecture, it is important to preprocess the data. For this assignment, the preprocessing has already been done for you.

Here's the process:
1. The data is tokenized with [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.WordPunctTokenizer)  which splits text based on word boundaries and punctuation.

2. Then we apply [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt). The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.
"""

from nltk.tokenize import WordPunctTokenizer
from subword_nmt.learn_bpe import learn_bpe
from subword_nmt.apply_bpe import BPE
tokenizer = WordPunctTokenizer()
def tokenize(x):
    return ' '.join(tokenizer.tokenize(x.lower()))

# split and tokenize the data
for lang in [source_lang, target_lang]:
    for split in splits:
        with open(split + '.tok.' + lang, 'w') as f_src:
            for line in open(split + '.' + lang):
                f_src.write(tokenize(line) + '\n')

# build and apply bpe vocs
bpe = {}
for lang in [source_lang, target_lang]:
    learn_bpe(open('./train.tok.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)
    bpe[lang] = BPE(open('./bpe_rules.' + lang))

    for split in splits:
        with open(split + '.bpe.' + lang, 'w') as f_out:
            for line in open(split + '.tok.' + lang):
                f_out.write(bpe[lang].process_line(line.strip()) + '\n')

"""### Building vocabularies

We now need to build vocabularies that map strings to token ids and vice versa. We're will use them when we feed training data into model or convert output matrices into words.
"""

train_inp = np.array(open('train.bpe.' + source_lang, encoding='utf-8').read().split('\n'))
train_out = np.array(open('train.bpe.' + target_lang, encoding='utf-8').read().split('\n'))

val_inp   = np.array(open('val.bpe.' + source_lang, encoding='utf-8').read().split('\n'))
val_out   = np.array(open('val.bpe.' + target_lang, encoding='utf-8').read().split('\n'))

test_inp = np.array(open('test.bpe.' + source_lang, encoding='utf-8').read().split('\n'))
test_out = np.array(open('test.bpe.' + target_lang, encoding='utf-8').read().split('\n')) # UPDATE 2025-02-26: Previous version incorrectly computed test BLEU score, now fixed

for i in range(3):
    print('inp:', train_inp[i])
    print('out:', train_out[i], end='\n\n')

from vocab import Vocab
inp_voc = Vocab.from_lines(train_inp)
out_voc = Vocab.from_lines(train_out)

print("Source vocab size", len(inp_voc))
print("Target vocab size", len(out_voc))

# Here's how you cast lines into ids and backwards.
batch_lines = sorted(train_inp, key=len)[5:10]
batch_ids = inp_voc.to_matrix(batch_lines)
batch_lines_restored = inp_voc.to_lines(batch_ids)

print("lines")
print(batch_lines)
print("\nwords to ids (0 = bos, 1 = eos):")
print(batch_ids)
print("\nback to words")
print(batch_lines_restored)

"""Draw source and translation length distributions to estimate the scope of the task."""

plt.figure(figsize=[8, 4])
plt.subplot(1, 2, 1)
plt.title("source length")
plt.hist(list(map(len, map(str.split, train_inp))), bins=20);

plt.subplot(1, 2, 2)
plt.title("translation length")
plt.hist(list(map(len, map(str.split, train_out))), bins=20);

"""### ❓ Question (5 points)

What tokens correspond to indices 0 and 1 in vocabulary? What roles do they play? Is it possible to train a model without them, and if so, how?

*<Your answer in 3-5 sentences>*

## 2. Basic Encoder-Decoder Model (15 points)

The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything. You need to implement ```decode_step``` (5 points).
"""

class BasicModel(nn.Module):
    def __init__(self, inp_voc, out_voc, emb_size=64, hid_size=128):
        """
        A simple encoder-decoder seq2seq model
        """
        super().__init__() # initialize base class to track sub-layers, parameters, etc.

        self.inp_voc, self.out_voc = inp_voc, out_voc
        self.hid_size = hid_size

        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)
        self.emb_out = nn.Embedding(len(out_voc), emb_size)
        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)

        self.dec_start = nn.Linear(hid_size, hid_size)
        self.dec0 = nn.GRUCell(emb_size, hid_size)
        self.logits = nn.Linear(hid_size, len(out_voc))

    def forward(self, inp, out):
        """ Apply model in training mode """
        initial_state = self.encode(inp)
        return self.decode(initial_state, out)


    def encode(self, inp, **flags):
        """
        Takes symbolic input sequence, computes initial state
        :param inp: matrix of input tokens [batch, time]
        :returns: initial decoder state tensors, one or many
        """
        # compute embeddings
        inp_emb = self.emb_inp(inp)
        batch_size = inp.shape[0]

        # sequentially applying a GRU cell to each input
        # (this logic is hidden in the GRU implementation)
        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)
        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]

        # note: enc0 works on a batch level, so we will need to process padding!
        # last_state is not _actually_ last because of padding, let's find the real last_state
        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)
        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]
        # ^-- shape: [batch_size, hid_size]

        # create an initial hidden vector for the decoder
        dec_start = self.dec_start(last_state)
        return [dec_start]

    def decode_step(self, prev_state, prev_tokens, **flags):
        """
        Takes previous decoder state and tokens, returns new state and logits for next tokens
        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)
        :param prev_tokens: previous output tokens, an int vector of [batch_size]
        :return: a list of next decoder state tensors, a tensor of logits [batch, len(out_voc)]
        """
        prev_gru0_state = prev_state[0]

        ### YOUR CODE
        # Get embedding of previous tokens
        prev_emb = self.emb_out(prev_tokens)

        # Apply decoder RNN cell
        new_dec_state = self.dec0(prev_emb, prev_gru0_state)

        # Get logits
        output_logits = self.logits(new_dec_state)
        ### END OF YOUR CODE

        return [new_dec_state], output_logits

    def decode(self, initial_state, out_tokens, **flags):
        """ Iterate over reference tokens (out_tokens) with decode_step, use for training"""
        batch_size = out_tokens.shape[0]
        state = initial_state

        # initial logits: always predict BOS
        onehot_bos = F.one_hot(torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64),
                               num_classes=len(self.out_voc)).to(device=out_tokens.device)
        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)

        logits_sequence = [first_logits]
        for i in range(out_tokens.shape[1] - 1):
            # note: we use the reference labels here (out_tokens)
            state, logits = self.decode_step(state, out_tokens[:, i])
            logits_sequence.append(logits)
        return torch.stack(logits_sequence, dim=1)

    def decode_inference(self, initial_state, max_len=100, **flags):
        """ Generate translations from model (greedy version), use for inference """
        batch_size, device = len(initial_state[0]), initial_state[0].device
        state = initial_state
        outputs = [torch.full([batch_size], self.out_voc.bos_ix, dtype=torch.int64,
                              device=device)]
        all_states = [initial_state]

        for i in range(max_len):
            # note: we use the predicted tokens
            state, logits = self.decode_step(state, outputs[-1])
            outputs.append(logits.argmax(dim=-1))
            all_states.append(state)

        return torch.stack(outputs, dim=1), all_states

    def translate_lines(self, inp_lines, **kwargs):
        inp = self.inp_voc.to_matrix(inp_lines).to(device)
        initial_state = self.encode(inp)
        out_ids, states = self.decode_inference(initial_state, **kwargs)
        return self.out_voc.to_lines(out_ids.cpu().numpy()), states

# debugging area
model = BasicModel(inp_voc, out_voc).to(device)

dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)
dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)

h0 = model.encode(dummy_inp_tokens)
h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))

assert isinstance(h1, list), type(h1)
assert len(h1) == len(h0)
assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])
assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))

logits_seq = model.decode(h0, dummy_out_tokens)
assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))

# full forward
logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)
assert logits_seq2.shape == logits_seq.shape

dummy_translations, dummy_states = model.translate_lines(train_inp[:3], max_len=25)
print("Translations without training:")
print('\n'.join([line for line in dummy_translations]))

"""### Training loss (5 points)

Our training objective is:
$$ L = {\frac1{|D|}} \sum_{X, Y \in D} \sum_{y_t \in Y} - \log p(y_t \mid y_1, \dots, y_{t-1}, X, \theta) $$

where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD.
"""

def compute_loss(model, inp, out, **flags):
    """
    Compute loss (float32 scalar) as in the formula above
    :param inp: input tokens matrix, int32[batch, time]
    :param out: reference tokens matrix, int32[batch, time]

    In order to pass the tests, your function should
    * include loss at first EOS but not the subsequent ones
    * divide sum of losses by a sum of input lengths (use voc.compute_mask)
    """
    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]
    targets_1hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)

    # outputs of the model, [batch_size, out_len, num_tokens]
    ### YOUR CODE
    outputs = model(inp, out)
    ### END OF YOUR CODE

    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]
    ### YOUR CODE
    log_probs = F.log_softmax(outputs, dim=-1)
    ### END OF YOUR CODE

    # log-probabilities of correct outputs, [batch_size, out_len]
    ### YOUR CODE
    log_probs_correct = (log_probs * targets_1hot).sum(dim=-1)
    ### END OF YOUR CODE
    # ^-- this will select the probability of the actual next token.

    # average cross-entropy over tokens where mask == True
    ### YOUR CODE
    loss = -log_probs_correct[mask].mean()

    ### END OF YOUR CODE
    return loss

dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)
print("Loss:", dummy_loss)
assert np.allclose(dummy_loss.item(), 7.6, rtol=0.01, atol=0.01), "We're sorry for your loss"

# test autograd
dummy_loss.backward()
for name, param in model.named_parameters():
    assert param.grad is not None and abs(param.grad.max()) != 0, f"Param {name} received no gradients"

print("Test passed!")

"""### Evaluation: BLEU

Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.
"""

from nltk.translate.bleu_score import corpus_bleu
def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):
    """
    Estimates corpora-level BLEU score of model's translations given inp and reference out
    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu
    """
    with torch.no_grad():
        translations, _ = model.translate_lines(inp_lines, **flags)
        translations = [line.replace(bpe_sep, '') for line in translations]
        actual = [line.replace(bpe_sep, '') for line in out_lines]
        return corpus_bleu(
            [[ref.split()] for ref in actual],
            [trans.split() for trans in translations],
            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]
            ) * 100

compute_bleu(model, val_inp, val_out)

"""### Training loop (2+3 points)

Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update
"""

from IPython.display import clear_output
from tqdm import tqdm, trange

model = BasicModel(inp_voc, out_voc).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
batch_size = 32

print("Model size", sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6, "M")

"""Implement training loop (2 points):"""

def train(model, opt, batch_size, num_steps=20000, save_path="best_basemodel.pth"):
    metrics = {'train_loss': [], 'dev_bleu': [] }
    best_bleu = 0.0
    for _ in trange(num_steps):
        step = len(metrics['train_loss']) + 1
        batch_ix = np.random.randint(len(train_inp), size=batch_size)
        batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)
        batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)

        ### YOUR CODE
        loss_t = compute_loss(model, batch_inp, batch_out)
        opt.zero_grad()
        loss_t.backward()
        opt.step()
        ### END OF YOUR CODE

        metrics['train_loss'].append((step, loss_t.item()))

        if step % 100 == 0:
            dev_bleu = compute_bleu(model, val_inp, val_out)
            metrics['dev_bleu'].append((step, dev_bleu))

            if dev_bleu > best_bleu:
                best_bleu = dev_bleu
                torch.save(model.state_dict(), save_path)

            clear_output(True)
            plt.figure(figsize=(12,4))
            for i, (name, history) in enumerate(sorted(metrics.items())):
                plt.subplot(1, len(metrics), i + 1)
                plt.title(name)
                plt.plot(*zip(*history))
                plt.grid()
            plt.show()
            print("Mean loss=%.3f" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)
    return metrics

"""Train your model (3 points):"""

metrics = train(model, opt, batch_size)
# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)

bleu_scores = np.mean(metrics['dev_bleu'][-10:], axis=0)[1]
assert bleu_scores > 13, f"BLEU score should be > 13 on average, you got {bleu_scores}"

model.load_state_dict(torch.load("best_basemodel.pth", weights_only=True))
model.to(device)

final_bleu_score = compute_bleu(model, test_inp, test_out)  # UPDATE 2025-02-26: Previous version incorrectly computed test BLEU score, now fixed
f"BLEU score on the test set: {final_bleu_score:.3}"

"""## 3. Encoder-Decoder Model with Attention (15 points)

In this section we want you to improve over the basic model by implementing a simple attention mechanism.

This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__.

### Attention layer (6 points)

Here you will have to implement a layer that computes a simple additive attention:

Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,

* Compute scores with a 2-layer neural network:
$$s_t = linear_{out}(tanh(linear_{enc}(h^e_t) + linear_{dec}(h^d)))$$
* Get probabilities from scores:
$$ p_t = {{e ^ {s_t}} \over { \sum_\tau e^{s_\tau} }} $$

* Add up encoder states with probabilities to get __attention response__ (attention context vector):
$$ c = \sum_t p_t \cdot h^e_t $$
"""

class AttentionLayer(nn.Module):
    def __init__(self, name, enc_size, dec_size, hid_size, activ=torch.tanh):
        """ A layer that computes additive attention response and weights """
        super().__init__()
        self.name = name
        self.enc_size = enc_size # num units in encoder state
        self.dec_size = dec_size # num units in decoder state
        self.hid_size = hid_size # attention layer hidden units
        self.activ = activ       # attention layer hidden nonlinearity

        # create trainable paramteres
        self.linear_enc = nn.Parameter(torch.zeros(self.enc_size, self.hid_size), requires_grad=True)
        self.linear_dec = nn.Parameter(torch.zeros(self.dec_size, self.hid_size), requires_grad=True) # you will need a couple of these
        self.linear_out = nn.Parameter(torch.zeros(self.hid_size, 1), requires_grad=True)

        torch.nn.init.xavier_uniform_(self.linear_enc.data)
        torch.nn.init.xavier_uniform_(self.linear_dec.data)
        torch.nn.init.xavier_uniform_(self.linear_out.data)


    def forward(self, enc, dec, inp_mask):
        """
        Computes attention response and weights
        :param enc: encoder activation sequence, float32[batch_size, ninp, enc_size]
        :param dec: single decoder state used as "query", float32[batch_size, dec_size]
        :param inp_mask: mask on enc activatons (0 after first eos), float32 [batch_size, ninp]
        :returns: attn[batch_size, enc_size], probs[batch_size, ninp]
            - attn - attention response vector (weighted sum of enc)
            - probs - attention weights after softmax
        """
        # Compute scores
        ### YOUR CODE
        scores = self.activ(torch.matmul(enc, self.linear_enc) + torch.matmul(dec.unsqueeze(1), self.linear_dec))
        scores = torch.matmul(scores, self.linear_out).squeeze(-1)
        ### END OF YOUR CODE

        # Apply mask - if mask is 0, scores should be -inf or -1e9
        # You may need torch.where
        ### YOUR CODE
        scores = torch.where(inp_mask, scores, torch.tensor(-1e9, device=scores.device))
        ### END OF YOUR CODE

        # Compute attention probabilities (softmax)
        ### YOUR CODE
        probs = torch.softmax(scores, dim=-1)
        ### END OF YOUR CODE

        # Compute attention response using enc and probs
        ### YOUR CODE
        attn = torch.sum(enc * probs.unsqueeze(-1), dim=1)
        ### END OF YOUR CODE

        return attn, probs

# debugging
batch_size = 32
ninp = 7
enc_size = 64
dec_size = 128
layer = AttentionLayer("", enc_size, dec_size, 72)
attn, probs = layer(torch.rand(batch_size, ninp, enc_size), torch.rand(batch_size, dec_size), torch.rand(batch_size, ninp) > .5)
assert attn.shape == (batch_size, enc_size), attn.shape
assert probs.shape == (batch_size, ninp), probs.shape # if you have redundant dimensions of size 1, you might find torch.squeeze useful

"""### Seq2seq model with attention (6 points)

You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:
![img](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bahdanau_model-min.png)

- On every step, use __previous__ decoder state to obtain attention response.
- Then feed concat this response to the inputs of next attention layer.

The key implementation detail here is __model state__. Any tensor added into the list of `encode` outputs list will be available for use during each `decode` step. You should include:
* Last RNN hidden states (as in basic model)
* The whole sequence of encoder outputs (to attend to) and mask
* Attention response and attention probabilities (to visualize)
"""

class AttentiveModel(BasicModel):
    def __init__(self, name, inp_voc, out_voc,
                 emb_size=64, hid_size=128, attn_size=128):
        """ Translation model that uses attention. Inherit all functions and params of BasicModel """
        self.inp_voc, self.out_voc = inp_voc, out_voc
        self.hid_size = hid_size

        super().__init__(inp_voc, out_voc, emb_size=emb_size, hid_size=hid_size)
        self.attention = AttentionLayer(name, hid_size, hid_size, attn_size)
        self.dec0 = nn.GRUCell(emb_size + attn_size, hid_size)

    def encode(self, inp, **flags):
        """
        Takes symbolic input sequence, computes initial state
        :param inp: matrix of input tokens [batch, time]
        :return: a list of initial decoder state tensors
        """
        # encode input sequence, create initial decoder states
        ### YOUR CODE
        inp_emb = self.emb_inp(inp)

        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)
        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]
        ### END OF YOUR CODE

        # note: last_state is not _actually_ last because of padding, let's find the real last_state
        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)
        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]
        # ^-- shape: [batch_size, hid_size]

        dec_start = self.dec_start(last_state)

        # apply attention layer from initial decoder hidden state
        inp_mask = self.inp_voc.compute_mask(inp)

        ### YOUR CODE
        first_attn_response, first_attn_probas = self.attention(enc_seq, dec_start, inp_mask)
        ### END OF YOUR CODE

        # Build first state: include
        # * initial states for decoder recurrent layers
        # * encoder sequence and encoder attn mask (for attention)
        # * make sure that last state item is attention probabilities tensor

        first_state = [dec_start, enc_seq, inp_mask, first_attn_response, first_attn_probas]
        return first_state

    def decode_step(self, prev_state, prev_tokens, **flags):
        """
        Takes previous decoder state and tokens, returns new state and logits for next tokens
        :param prev_state: a list of previous decoder state tensors
        :param prev_tokens: previous output tokens, an int vector of [batch_size]
        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]
        """
        prev_gru0_state, enc_seq, inp_mask, prev_attn_response, prev_attn_probas = prev_state

        ### YOUR CODE
        prev_tokens_emb = self.emb_out(prev_tokens)
        concat_inp = torch.cat([prev_tokens_emb, prev_attn_response], dim=-1)
        new_dec_state = self.dec0(concat_inp, prev_gru0_state)
        output_logits = self.logits(new_dec_state)

        attn_response, attn_probas = self.attention(enc_seq, new_dec_state, inp_mask)
        ### END OF YOUR CODE

        return [new_dec_state, enc_seq, inp_mask, attn_response, attn_probas], output_logits

"""### Training attentive model (3 points)

Please reuse the infrastructure you've built for the basic model.
"""

metrics = {'train_loss': [], 'dev_bleu': [] }

model = AttentiveModel("model", inp_voc, out_voc, hid_size=128, attn_size=128).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
batch_size = 32

metrics = train(model, opt, batch_size, save_path="best_attentivemodel.pth")

model.load_state_dict(torch.load("best_attentivemodel.pth", weights_only=True))
model.to(device)

final_bleu_score = compute_bleu(model, test_inp, test_out)  # UPDATE 2025-02-26: Previous version incorrectly computed test BLEU score, now fixed
f"BLEU score on the test set: {final_bleu_score:.3}"

"""### ❓ Question

Compare the BLEU scores of the attentive model and the basic model you trained in the previous section. Which one performs better, and why?

The attentive model performed better than the basic model because the attention mechanism allows the decoder to focus on different parts of the input sequence at each step of the output generation. This helps the model to better capture the alignment between the source and target languages, leading to more accurate translations. The attention mechanism effectively provides a dynamic context that improves the model's ability to handle long sentences and complex structures.

## 4. Analyzing NMT (10 points)

### Translation Analysis (4 points)

Let's explore how the model we trained translates:
"""

for inp_line in test_inp[:5]:
    trans_line, _ = model.translate_lines([inp_line])
    print(inp_line)
    print(trans_line[0])
    print()

"""### ❓ Questions

1. Analyze the types of errors your model makes.  You may use any external translator (e.g., Google Translate) to translate your German predictions back to English to better understand their meaning.

*<Your answer in 1-3 sentences>*

2. What can help improve these translations?

*<Your answer in 1-3 sentences>*

### Visualizing model attention (6 points)

You can validate your model by visualizing its attention weights. We provided you with a function that draws attention maps.
"""

import matplotlib.pyplot as plt
import numpy as np

def draw_attention(inp_line, translation, probs):
    """ Visualize attention weights with a black background and white squares """
    # Tokenize input and translation
    inp_tokens = inp_voc.tokenize(inp_line)
    trans_tokens = out_voc.tokenize(translation)
    probs = probs[:len(trans_tokens), :len(inp_tokens)]

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.set_facecolor("black")
    cax = ax.matshow(probs[::-1], cmap='gray')

    color_bar = fig.colorbar(cax, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)

    ax.set_xticks(np.arange(len(inp_tokens)))
    ax.set_xticklabels(inp_tokens, rotation=45, ha='left', rotation_mode='anchor')

    ax.set_yticks(np.arange(len(trans_tokens)))
    ax.set_yticklabels(trans_tokens[::-1])


    ax.tick_params(axis='both', which='both', length=0)
    ax.spines[:].set_visible(False)

    plt.show()

trans, states = model.translate_lines(test_inp[:5])

# select attention probs from model state
# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states
# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)

### YOUR CODE
attention_probs = np.stack([state[-1].detach().cpu().numpy() for state in states], axis=1)
### END OF YOUR CODE

# Check for all examples and all translation steps
assert np.allclose(attention_probs.sum(axis=2), 1.0, atol=1e-4), "Attention probabilities do not sum to 1!"

for i in range(5):
    draw_attention(test_inp[i], trans[i], attention_probs[i])
# Does it look fine already? don't forget to save images!

"""### ❓ Question

Interpret how attention works:

*<Your answer in 3-5 sentences>*
"""
